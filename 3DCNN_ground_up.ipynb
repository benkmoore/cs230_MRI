{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ground up 3D neural network for ADNI alzheimers classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import json\n",
    "\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "import scipy as scipy\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.engine import Layer\n",
    "\n",
    "from keras.layers import Input, Dense, Conv1D, Conv2D, MaxPooling2D, Deconvolution2D, UpSampling2D, Reshape, Flatten, ZeroPadding2D, BatchNormalization, Lambda, Dropout, Activation\n",
    "from keras.layers import Conv3D, MaxPooling3D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#from vis.utils import utils\n",
    "#from vis.visualization import visualize_saliency\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import nibabel as nib\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from keras.regularizers import l2\n",
    "\n",
    "theano.config.opennp = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters (Modify as needed)\n",
    "img_size_x = 96\n",
    "img_size_y = 96\n",
    "img_size_z = 62\n",
    "\n",
    "batch_size = 8\n",
    "nb_classes = 3\n",
    "nb_epoch = 50\n",
    "\n",
    "c = 0\n",
    "\n",
    "early_stopping_patience = 20\n",
    "\n",
    "class_names = [\"CN\", \"MCI\", \"AD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_slices(slices):\n",
    "    \"\"\" Function to display row of image slices \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(slices))\n",
    "    for i, slice in enumerate(slices):\n",
    "        axes[i].imshow(slice.T, cmap=\"gray\", origin=\"lower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dont use this one\n",
    "def load_dataset(directory):\n",
    "    \n",
    "    patientData = np.loadtxt(\"ADNI1_Complete_1Yr_1.5T_10_26_2019.csv\", \n",
    "                         dtype= 'str', skiprows=1, delimiter=',')\n",
    "    \n",
    "    integer_mapping = {x: i for i,x in enumerate(['AD', 'MCI', 'CN'])}\n",
    "    y = np.asarray([integer_mapping[word] for word in patientData[:,1]])\n",
    "    labels = to_categorical(y, num_classes=3, dtype='float32')\n",
    "    \n",
    "    \n",
    "    \n",
    "    xdim = 96\n",
    "    ydim = 96\n",
    "    zdim = 62\n",
    "    X = np.zeros((306,xdim,ydim,zdim,1))\n",
    "    Y = np.zeros(306)\n",
    "    \n",
    "    l = os.listdir(directory)\n",
    "    random.shuffle(l)\n",
    "    \n",
    "    countHC = 0\n",
    "    countMCI = 0\n",
    "    countAD = 0\n",
    "    \n",
    "    objindex = 0\n",
    "    for i, filename in enumerate(l):\n",
    "        if (filename.startswith('.')):\n",
    "            print (\"hidden file\")\n",
    "        else:\n",
    "            subject = filename[:-4]\n",
    "            \n",
    "            ind = np.where(patientData[:,0] == subject[:-1])[0][0]\n",
    "            y = labels[ind, :]\n",
    "            y = y.reshape(1,-1)\n",
    "\n",
    "            if str(y) == \"[[1. 0. 0.]]\":\n",
    "                y = 1\n",
    "            if str(y) == \"[[0. 1. 0.]]\":\n",
    "                continue\n",
    "            if str(y) == \"[[0. 0. 1.]]\":\n",
    "                y = 0\n",
    "            \n",
    "            Y[objindex] = y\n",
    "            objindex += 1\n",
    "\n",
    "            epi_img = nib.load(directory + subject + '.nii')\n",
    "            # Get voxel array\n",
    "            epi_img_data = epi_img.get_fdata()\n",
    "            n_i, n_j, n_k = epi_img_data.shape\n",
    "\n",
    "            #if (n_i != 192 or n_j != 192):\n",
    "                #epi_img_data = cv2.resize(epi_img_data, (192, 192))\n",
    "\n",
    "            '''\n",
    "            if (n_i != 192 or n_j != 192 or n_k != 160):\n",
    "                epi_img_data = cv2.resize(epi_img_data, (192, 192))\n",
    "                #epi_img_data = epi_img_data[0:192,0:192,int(n_k/2)-80:int(n_k/2)+80]\n",
    "\n",
    "                #resampling to make all MRI volumes the same dimensions\n",
    "                epi_img_data = zoom(epi_img_data, (float(xdim/n_i), float(ydim/n_j), float(zdim/n_k)), order = 0)'''\n",
    "\n",
    "            x = zoom(epi_img_data, (float(xdim/n_i), float(ydim/n_j), float(zdim/n_k)), order = 1)\n",
    "            \n",
    "            x = (x-x.min())/x.max()\n",
    "\n",
    "            #x = epi_img_data[:, :, int((n_k-1)/2)]\n",
    "\n",
    "            x = np.expand_dims(x, axis=3)\n",
    "\n",
    "            X[objindex] = x\n",
    "\n",
    "            \n",
    "\n",
    "            if objindex % 50 == 0:\n",
    "                print (\"loaded \" + str(objindex) + \"subjects\")\n",
    "\n",
    "                print (subject + \" \" + str(y))\n",
    "        \n",
    "        \n",
    "    print (X.shape)\n",
    "    print (Y.shape)\n",
    "    \n",
    "    print (countHC)\n",
    "    print (countMCI)\n",
    "    print (countAD)\n",
    "        \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dimension = '3d'):\n",
    "    \n",
    "    def load_mri_images(filename):\n",
    "        global c\n",
    "        \n",
    "        data = np.load(filename)\n",
    "        \n",
    "        tmp = c\n",
    "        c = tmp + 1\n",
    "        \n",
    "        print ('Loaded image set %d of 32.' %c)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def imgwise_2d_scaling(data):\n",
    "        #loop over patients\n",
    "        for i in range(len(data)):\n",
    "            for j in range(len(data[i][0])):\n",
    "                max_val_2d = np.amax(data[i][0][j])\n",
    "\n",
    "                data[i][0][j] = data[i][0][j].astype('float32')\n",
    "                data[i][0][j] /= max_val_2d\n",
    "                \n",
    "        print ('Executed imagewise 2d scaling.')\n",
    "\n",
    "        return data\n",
    "\n",
    "    def imgwise_3d_scaling(data):\n",
    "         #loop over patients\n",
    "        for i in range(len(data)):\n",
    "            max_val_3d = np.amax(data[i][0])\n",
    "\n",
    "            data[i][0] = data[i][0].astype('float32')\n",
    "            data[i][0] /= max_val_3d\n",
    "\n",
    "        print ('Executed imagewise 3d scaling.')\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def reshape_mri_images(data):\n",
    "        #Reshape the loaded dataset to the appropriate format.\n",
    "        data = np.expand_dims(data,axis=1)\n",
    "        \n",
    "        if(dimension == '3d'):\n",
    "            \n",
    "            data = np.reshape(data, (-1, img_size_z, img_size_x, img_size_y, 1))\n",
    "        \n",
    "        print ('Reshaped images.')\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def load_mri_labels(filename, train_valid_test):\n",
    "        data = pd.read_csv(filename)\n",
    "\n",
    "        data = data.loc[data['train_valid_test'] == train_valid_test]\n",
    "        \n",
    "        data = np.asarray(data.diagnosis)\n",
    "        data_new = np.array([])\n",
    "        \n",
    "        if(dimension == '2d'):\n",
    "            for i, item in enumerate(data):\n",
    "                data_temp = np.array([])\n",
    "                for i in range(img_size_z):\n",
    "                    data_temp = np.append(data_temp, item)  \n",
    "                \n",
    "                data_new = np.append(data_new, data_temp)\n",
    "        else:\n",
    "            data_new = data\n",
    "            \n",
    "        data_new = data_new.reshape((-1, 1))\n",
    "        data_new = data_new.astype(np.int64)\n",
    "        \n",
    "        #labels start at 1, normalise them to start at 0.\n",
    "        data_new = np.subtract(data_new, 1)\n",
    "        \n",
    "        data_new = np_utils.to_categorical(data_new, nb_classes)\n",
    "        \n",
    "        print ('Loaded labels.')\n",
    "\n",
    "        return data_new\n",
    "    \n",
    "    train_data = load_mri_images('Data/img_array_train_6k_1.npy')\n",
    "    for i in range(2,23):\n",
    "        train_cur = load_mri_images('Data/img_array_train_6k_%d.npy' %i)\n",
    "        train_data = np.vstack((train_data, train_cur))\n",
    "    train_data = reshape_mri_images(train_data)\n",
    "    \n",
    "    val_data = load_mri_images('Data/img_array_valid_6k_1.npy')\n",
    "    for i in range(2,6):\n",
    "        valid_cur = load_mri_images('Data/img_array_valid_6k_%d.npy' %i)\n",
    "        val_data = np.vstack((val_data, valid_cur))\n",
    "    val_data = reshape_mri_images(val_data)\n",
    "    \n",
    "    test_data = load_mri_images('Data/img_array_test_6k_1.npy')\n",
    "    for i in range(2,6):\n",
    "        test_cur = load_mri_images('Data/img_array_test_6k_%d.npy' %i)\n",
    "        test_data = np.vstack((test_data, test_cur))\n",
    "    test_data = reshape_mri_images(test_data)\n",
    "    \n",
    "    if(dimension == '3d'):\n",
    "        train_data = imgwise_3d_scaling(train_data)\n",
    "        val_data = imgwise_3d_scaling(val_data)\n",
    "        test_data = imgwise_3d_scaling(test_data)\n",
    "    else:\n",
    "        train_data = imgwise_2d_scaling(train_data)\n",
    "        val_data = imgwise_2d_scaling(val_data)\n",
    "        test_data = imgwise_2d_scaling(test_data)\n",
    "        \n",
    "    train_labels = load_mri_labels('adni_demographic_master_kaggle.csv', 0)\n",
    "    val_labels = load_mri_labels('adni_demographic_master_kaggle.csv', 1)\n",
    "    test_labels = load_mri_labels('adni_demographic_master_kaggle.csv', 2)\n",
    "    \n",
    "    # remove MCI data\n",
    "    print(train_labels.shape)\n",
    "    train_labels = np.delete(train_labels,1,axis=1)\n",
    "    print(train_labels.shape)\n",
    "    test_labels =  np.delete(test_labels,1,axis=1)\n",
    "    val_labels =  np.delete(val_labels,1,axis=1)\n",
    "    \n",
    "    train_labels = np.where(train_labels[:,:]==[0,0],[1,0],train_labels[:,:])\n",
    "    test_labels = np.where(test_labels[:,:]==[0,0],[1,0],test_labels[:,:])\n",
    "    val_labels = np.where(val_labels[:,:]==[0,0],[1,0],val_labels[:,:])\n",
    "    \n",
    "    print ('Done.')\n",
    "    \n",
    "    return train_data, train_labels, test_data, test_labels, val_data, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image set 1 of 32.\n",
      "Loaded image set 2 of 32.\n",
      "Loaded image set 3 of 32.\n",
      "Loaded image set 4 of 32.\n",
      "Loaded image set 5 of 32.\n",
      "Loaded image set 6 of 32.\n",
      "Loaded image set 7 of 32.\n",
      "Loaded image set 8 of 32.\n",
      "Loaded image set 9 of 32.\n",
      "Loaded image set 10 of 32.\n",
      "Loaded image set 11 of 32.\n",
      "Loaded image set 12 of 32.\n",
      "Loaded image set 13 of 32.\n",
      "Loaded image set 14 of 32.\n",
      "Loaded image set 15 of 32.\n",
      "Loaded image set 16 of 32.\n",
      "Loaded image set 17 of 32.\n",
      "Loaded image set 18 of 32.\n",
      "Loaded image set 19 of 32.\n",
      "Loaded image set 20 of 32.\n",
      "Loaded image set 21 of 32.\n",
      "Loaded image set 22 of 32.\n",
      "Reshaped images.\n",
      "Loaded image set 23 of 32.\n",
      "Loaded image set 24 of 32.\n",
      "Loaded image set 25 of 32.\n",
      "Loaded image set 26 of 32.\n",
      "Loaded image set 27 of 32.\n",
      "Reshaped images.\n",
      "Loaded image set 28 of 32.\n",
      "Loaded image set 29 of 32.\n",
      "Loaded image set 30 of 32.\n",
      "Loaded image set 31 of 32.\n",
      "Loaded image set 32 of 32.\n",
      "Reshaped images.\n",
      "Executed imagewise 3d scaling.\n",
      "Executed imagewise 3d scaling.\n",
      "Executed imagewise 3d scaling.\n",
      "Loaded labels.\n",
      "Loaded labels.\n",
      "Loaded labels.\n",
      "(2109, 3)\n",
      "(2109, 2)\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels, test_data, test_labels, val_data, val_labels = load_dataset(dimension = '3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n",
      "(2109, 62, 96, 96, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[0:10])\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Dont run\n",
    "\n",
    "X, Y = load_dataset(directory = \"/home/ubuntu/project/Data/CombinedClean/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dont run\n",
    "\n",
    "train_data = train_data[0:10]\n",
    "train_labels = train_labels[0:10]\n",
    "#test_data = X[205:245]\n",
    "#test_labels = Y[205:245]\n",
    "#val_data = X[205:306] #change\n",
    "#val_labels = Y[205:306] #change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2109, 62, 96, 96, 1)\n"
     ]
    }
   ],
   "source": [
    "print (train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class roc_callback(Callback):\n",
    "    def __init__(self,training_data,validation_data):\n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.x)\n",
    "        roc = roc_auc_score(self.y, y_pred)\n",
    "        y_pred_val = self.model.predict(self.x_val)\n",
    "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
    "        print('\\rroc-auc: %s - roc-auc_val: %s' % (str(round(roc,4)),str(round(roc_val,4))),end=100*' '+'\\n')\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Builds keras 3D CNN model\n",
    "def build_cnn(dimension = '3d', activation = 'softmax', heatmap = False, w_path = None, compile_model = True):\n",
    "    input_3d = (img_size_z, img_size_x, img_size_y, 1)\n",
    "    input_2d = (1, img_size_x, img_size_y)\n",
    "    \n",
    "    pool_3d = (2, 2, 2)\n",
    "    pool_2d = (2, 2)\n",
    "    \n",
    "    def global_average_pooling(x):\n",
    "        return K.mean(x, axis = (2, 3))\n",
    "\n",
    "    def global_average_pooling_shape(input_shape):\n",
    "        return input_shape[0:2]\n",
    "    \n",
    "    def build_conv_3d():\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Conv3D(8, (3, 3, 3), name='conv1', input_shape=input_3d))\n",
    "        model.add(MaxPooling3D(pool_size=pool_3d, name='pool1'))\n",
    "        \n",
    "        model.add(Conv3D(8, (3, 3, 3), name='conv2', input_shape=input_3d))\n",
    "        model.add(MaxPooling3D(pool_size=pool_3d, name='pool2'))\n",
    "\n",
    "        model.add(Conv3D(8, (3, 3, 3), name='conv3'))\n",
    "        model.add(MaxPooling3D(pool_size=pool_3d, name='pool3'))\n",
    "        \n",
    "        #model.add(Convolution3D(32, 3, 3, 3, name='conv2b'))\n",
    "        #model.add(MaxPooling3D(pool_size=pool_3d, name='pool2b'))\n",
    "\n",
    "        #model.add(Convolution3D(32, 3, 3, 3, name='conv3a'))\n",
    "        #model.add(MaxPooling3D(pool_size=pool_3d, name='pool3a'))\n",
    "        \n",
    "        #model.add(Convolution3D(16, 3, 3, 3, name='conv4'))\n",
    "        #model.add(MaxPooling3D(pool_size=pool_3d, name='pool4'))\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def build_conv_2d():\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Convolution2D(8, 3, 3, name='conv1', input_shape=input_2d))\n",
    "        model.add(MaxPooling2D(pool_size=pool_2d, name='pool1'))\n",
    "\n",
    "        model.add(Convolution2D(8, 3, 3, name='conv2'))\n",
    "        model.add(MaxPooling2D(pool_size=pool_2d, name='pool2'))\n",
    "\n",
    "        model.add(Convolution2D(8, 3, 3, name='conv3'))\n",
    "        model.add(MaxPooling2D(pool_size=pool_2d, name='pool3'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    if(dimension == '3d'):\n",
    "        model = build_conv_3d()\n",
    "    else:\n",
    "        model = build_conv_2d()\n",
    "        \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2000, activation='relu', name='dense1'))\n",
    "    model.add(Dropout(0.5, name='dropout1'))\n",
    "    \n",
    "    #model.add(Dense(2000, activation='relu', name='dense1a'))\n",
    "    #model.add(Dropout(0.5, name='dropout1a'))\n",
    "\n",
    "    model.add(Dense(500, activation='relu', name='dense2'))\n",
    "    model.add(Dropout(0.5, name='dropout2'))\n",
    "\n",
    "    model.add(Dense(2, activation='softmax', name='softmax'))\n",
    "    \n",
    "    '''model = Sequential()\n",
    "    # 1st Volumetric Convolutional block\n",
    "    model.add(Conv3D(8, (3, 3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01), input_shape=input_3d))\n",
    "    model.add(Conv3D(8, (3, 3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01)))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "    # 2nd Volumetric Convolutional block\n",
    "    model.add(Conv3D(16, (3, 3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Conv3D(16, (3, 3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01)))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "    # 3rd Volumetric Convolutional block\n",
    "    model.add(Conv3D(32, (3, 3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Conv3D(32, (3, 3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Conv3D(32, (3, 3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01)))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "    # 4th Volumetric Convolutional block\n",
    "    model.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01)))\n",
    "    model.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01)))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "    model.add(Flatten())\n",
    "    # 1th Deconvolutional layer with batchnorm and dropout for regularization\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.7))\n",
    "    # 2th Deconvolutional layer\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    # Output with softmax nonlinearity for classification\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "'''\n",
    "    if w_path:\n",
    "        model.load_weights(w_path)\n",
    "        \n",
    "    #opt = keras.optimizers.Adam(lr=1e-9, beta_1=0.9, beta_2=0.99, amsgrad=False)\n",
    "\n",
    "    #opt = keras.optimizers.Adadelta(lr=1e-5)#clipnorm=1.)\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=1e-6,beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    \n",
    "    #opt = SGD(lr=0.0000001)\n",
    "    \n",
    "    if(compile_model):\n",
    "        model.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    print ('Done building model.')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done building model.\n"
     ]
    }
   ],
   "source": [
    "model = build_cnn(dimension = '3d', compile_model = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv3D)               (None, 60, 94, 94, 8)     224       \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling3D)         (None, 30, 47, 47, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv3D)               (None, 28, 45, 45, 8)     1736      \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling3D)         (None, 14, 22, 22, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv3D)               (None, 12, 20, 20, 8)     1736      \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling3D)         (None, 6, 10, 10, 8)      0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4800)              0         \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 2000)              9602000   \n",
      "_________________________________________________________________\n",
      "dropout1 (Dropout)           (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 500)               1000500   \n",
      "_________________________________________________________________\n",
      "dropout2 (Dropout)           (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 2)                 1002      \n",
      "=================================================================\n",
      "Total params: 10,607,198\n",
      "Trainable params: 10,607,198\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracks model learning rate\n",
    "class SGDLearningRateTracker(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        optimizer = self.model.optimizer\n",
    "        lr = K.eval(optimizer.lr * (1. / (1. + optimizer.decay * optimizer.iterations)))\n",
    "        print (str('\\nLR: {:.6f}\\n').format(float(lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting model architecture to data, also runs loss/accuracy tracker\n",
    "def fit_model(model, v, train_data, train_labels, val_data, val_labels):\n",
    "    model_weights_file = 'img_classifier_weights_%s.h5' %v\n",
    "    epoch_weights_file = 'img_classifier_weights_%s_{epoch:02d}_{val_acc:.2f}.hdf5' %v\n",
    "    model_file = 'img_classifier_model_%s.h5' %v\n",
    "    history_file = 'img_classifier_history_%s.json' %v\n",
    "    \n",
    "    def save_model_and_weights():\n",
    "        model.save(model_file)\n",
    "        model.save_weights(model_weights_file)\n",
    "        \n",
    "        return 'Saved model and weights to disk!'\n",
    "\n",
    "    def save_model_history(m):\n",
    "        with open(history_file, 'w', encoding=\"utf8\") as history_json_file:\n",
    "            json.dump(m.history, history_json_file)\n",
    "        \n",
    "        return 'Saved model history to disk!'\n",
    "    \n",
    "    def visualise_accuracy(m):\n",
    "        plt.plot(m.history['acc'])\n",
    "        plt.plot(m.history['val_acc'])\n",
    "        plt.title('model accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'validation'], loc='upper left')\n",
    "        plt.show()\n",
    "      \n",
    "    def visualise_loss(m):\n",
    "        plt.plot(m.history['loss'])\n",
    "        plt.plot(m.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'validation'], loc='upper left')\n",
    "        plt.show()\n",
    "    \n",
    "    def model_callbacks():\n",
    "        checkpoint = ModelCheckpoint(epoch_weights_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=early_stopping_patience, verbose=1, mode='auto')\n",
    "        lr_tracker = SGDLearningRateTracker()\n",
    "        \n",
    "        #roc = roc_callback(training_data=(train_data, train_labels),validation_data=(val_data,val_labels))\n",
    "        \n",
    "        return [checkpoint]\n",
    "        \n",
    "    callbacks_list = model_callbacks()\n",
    "    \n",
    "    y_ints = [y.argmax() for y in train_labels]\n",
    "    \n",
    "    #class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 #np.unique(y_ints),\n",
    "                                                 #y_ints)\n",
    "    #print (class_weights)\n",
    "    \n",
    "    m = model.fit(train_data,train_labels,batch_size=batch_size, epochs=nb_epoch, verbose=1,shuffle=True,validation_data=(val_data,val_labels),callbacks=callbacks_list)\n",
    "    \n",
    "    print (save_model_and_weights())\n",
    "    print (save_model_history(m))\n",
    "    \n",
    "    visualise_accuracy(m)\n",
    "    visualise_loss(m)\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates test data performance and creates confusion matrix\n",
    "def evaluate_model(m, weights, test_data, test_labels):    \n",
    "    def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "\n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, cm[i, j],\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "     \n",
    "    plt.close('all')\n",
    "\n",
    "    m.load_weights(weights)\n",
    "    m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    print (\"Done compiling model.\")\n",
    "    \n",
    "    prediction = m.predict(test_data)\n",
    "    prediction_labels = np_utils.to_categorical(np.argmax(prediction, axis=1), nb_classes)\n",
    "    \n",
    "    #print (prediction)\n",
    "    \n",
    "    pred = []\n",
    "    for p in prediction:\n",
    "        if float(p[0]) > 0.5:\n",
    "            pred.append(1)\n",
    "        else:\n",
    "            pred.append(0)\n",
    "    print (pred)\n",
    "    \n",
    "    print ('Accuracy on test data:', accuracy_score(test_labels, prediction_labels))\n",
    "\n",
    "    print ('Classification Report')\n",
    "    print (classification_report(test_labels, prediction_labels, target_names = ['CN', 'AD']))\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(np.argmax(test_labels, axis=1), np.argmax(prediction, axis=1))\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes = class_names, normalize=False, title='Confusion matrix')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2109 samples, validate on 435 samples\n",
      "Epoch 1/50\n",
      "2109/2109 [==============================] - 49s 23ms/step - loss: 6.2851 - acc: 0.8857 - val_loss: 5.1133 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 1.00000, saving model to img_classifier_weights_v1_01_1.00.hdf5\n",
      "Epoch 2/50\n",
      "2109/2109 [==============================] - 48s 23ms/step - loss: 5.5401 - acc: 0.9559 - val_loss: 5.1133 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 1.00000\n",
      "Epoch 3/50\n",
      "2109/2109 [==============================] - 47s 22ms/step - loss: 5.4112 - acc: 0.9682 - val_loss: 5.1133 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 1.00000\n",
      "Epoch 4/50\n",
      "1584/2109 [=====================>........] - ETA: 10s - loss: 5.1716 - acc: 0.9811"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-8337222c0163>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#This will take time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-90a710509bc7>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model, v, train_data, train_labels, val_data, val_labels)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m#print (class_weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msave_model_and_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#This will take time\n",
    "m = fit_model(model, \"v1\", train_data, train_labels, val_data, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the fit model and evaluate performance on test\n",
    "evaluate_model(model, 'img_classifier_weights_v1.h5', test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

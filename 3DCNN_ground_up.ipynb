{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ground up 3D neural network for ADNI alzheimers classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import cv2\n",
    "\n",
    "import scipy as scipy\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import itertools\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.engine import Layer\n",
    "\n",
    "from keras.layers import Input, Dense, Convolution1D, Convolution2D, MaxPooling2D, Deconvolution2D, UpSampling2D, Reshape, Flatten, ZeroPadding2D, BatchNormalization, Lambda, Dropout, Activation\n",
    "from keras.layers import Convolution3D, MaxPooling3D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from vis.utils import utils\n",
    "#from vis.visualization import visualize_saliency\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import nibabel as nib\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters (Modify as needed)\n",
    "img_size_x = 192\n",
    "img_size_y = 192\n",
    "img_size_z = 160\n",
    "\n",
    "batch_size = 128\n",
    "nb_classes = 3\n",
    "nb_epoch = 25\n",
    "\n",
    "c = 0\n",
    "\n",
    "learning_rate = 0.003\n",
    "early_stopping_patience = 20\n",
    "\n",
    "class_names = [\"CN\", \"MCI\", \"AD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_slices(slices):\n",
    "    \"\"\" Function to display row of image slices \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(slices))\n",
    "    for i, slice in enumerate(slices):\n",
    "        axes[i].imshow(slice.T, cmap=\"gray\", origin=\"lower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(directory):\n",
    "    \n",
    "    patientData = np.loadtxt(\"ADNI1_Complete_1Yr_1.5T_10_26_2019.csv\", \n",
    "                         dtype= 'str', skiprows=1, delimiter=',')\n",
    "    \n",
    "    integer_mapping = {x: i for i,x in enumerate(['AD', 'MCI', 'CN'])}\n",
    "    y = np.asarray([integer_mapping[word] for word in patientData[:,1]])\n",
    "    labels = to_categorical(y, num_classes=3, dtype='float32')\n",
    "    \n",
    "    \n",
    "    \n",
    "    xdim = 192\n",
    "    ydim = 192\n",
    "    zdim = 160\n",
    "    X = np.zeros((637,xdim,ydim,zdim,1))\n",
    "    Y = np.zeros((637,3))\n",
    "    for i, filename in enumerate(os.listdir(directory)):\n",
    "        subject = filename[:-4]\n",
    "        \n",
    "        epi_img = nib.load(directory + subject + '.nii')\n",
    "        # Get voxel array\n",
    "        epi_img_data = epi_img.get_fdata()\n",
    "        n_i, n_j, n_k = epi_img_data.shape\n",
    "        \n",
    "        if (n_i != 192 or n_j != 192 or n_k != 160):\n",
    "            #epi_img_data = cv2.resize(epi_img_data, (192, 192))\n",
    "            #epi_img_data = epi_img_data[0:192,0:192,int(n_k/2)-80:int(n_k/2)+80]\n",
    "            \n",
    "            #resampling to make all MRI volumes the same dimensions\n",
    "            epi_img_data = zoom(epi_img_data, (float(xdim/n_i), float(ydim/n_j), float(zdim/n_k)), order = 0)\n",
    "        \n",
    "        x = epi_img_data\n",
    "        \n",
    "        x = np.expand_dims(x, axis=3)\n",
    "        \n",
    "        X[i] = x\n",
    "        \n",
    "        ind = np.where(patientData[:,0] == subject[:-1])[0][0]\n",
    "        y = labels[ind, :]\n",
    "        y = y.reshape(1,-1)\n",
    "        \n",
    "        Y[i] = y\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print (\"loaded \" + i + \"subjects\")\n",
    "        \n",
    "        \n",
    "    print (X.shape)\n",
    "    print (Y.shape)\n",
    "        \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 192, 160, 1)\n",
      "0\n",
      "(192, 192, 160, 1)\n",
      "1\n",
      "(192, 192, 160, 1)\n",
      "2\n",
      "(192, 192, 160, 1)\n",
      "3\n",
      "(192, 192, 160, 1)\n",
      "4\n",
      "(192, 192, 160, 1)\n",
      "5\n",
      "(192, 192, 160, 1)\n",
      "6\n",
      "(192, 192, 160, 1)\n",
      "7\n",
      "(192, 192, 160, 1)\n",
      "8\n",
      "(192, 192, 160, 1)\n",
      "9\n",
      "(192, 192, 160, 1)\n",
      "10\n",
      "(192, 192, 160, 1)\n",
      "11\n",
      "(192, 192, 160, 1)\n",
      "12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b557889056a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ADNI_TRAIN/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-2ffaa470171e>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mepi_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msubject\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.nii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Get voxel array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mepi_img_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepi_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mn_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_j\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepi_img_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nibabel/dataobj_images.py\u001b[0m in \u001b[0;36mget_fdata\u001b[0;34m(self, caching, dtype)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fdata_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fdata_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcaching\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'fill'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fdata_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X, Y = load_dataset(directory = \"ADNI_TRAIN/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, Y):\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=1)\n",
    "    \n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=1)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-142f3e0403cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-93-07eb0b00cc4d>\u001b[0m in \u001b[0;36msplit_dataset\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2123\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[0;32m-> 2124\u001b[0;31m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2123\u001b[0m     return list(chain.from_iterable((safe_indexing(a, train),\n\u001b[0;32m-> 2124\u001b[0;31m                                      safe_indexing(a, test)) for a in arrays))\n\u001b[0m\u001b[1;32m   2125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36msafe_indexing\u001b[0;34m(X, indices)\u001b[0m\n\u001b[1;32m    217\u001b[0m                                    indices.dtype.kind == 'i'):\n\u001b[1;32m    218\u001b[0m             \u001b[0;31m# This is often substantially faster than X[indices]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data, train_labels, test_data, test_labels, val_data, val_labels = split_dataset(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X[0:450]\n",
    "train_labels = Y[0:450]\n",
    "test_data = X[450:545]\n",
    "test_labels = Y[450:545]\n",
    "val_data = X[545:637]\n",
    "val_labels = Y[545:637]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Builds keras 3D CNN model\n",
    "def build_cnn(dimension = '3d', activation = 'softmax', heatmap = False, w_path = None, compile_model = True):\n",
    "    input_3d = (img_size_x, img_size_y, img_size_z, 1)\n",
    "    input_2d = (img_size_x, img_size_y, 1)\n",
    "    \n",
    "    pool_3d = (2, 2, 2)\n",
    "    pool_2d = (2, 2)\n",
    "    \n",
    "    def build_conv_3d():\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Convolution3D(8, 3, 3, 3, name='conv1', input_shape=input_3d))\n",
    "        model.add(MaxPooling3D(pool_size=pool_3d, name='pool1'))\n",
    "\n",
    "        model.add(Convolution3D(8, 3, 3, 3, name='conv2'))\n",
    "        model.add(MaxPooling3D(pool_size=pool_3d, name='pool2'))\n",
    "\n",
    "        model.add(Convolution3D(8, 3, 3, 3, name='conv3'))\n",
    "        model.add(MaxPooling3D(pool_size=pool_3d, name='pool3'))\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def build_conv_2d():\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Convolution2D(8, 3, 3, name='conv1', input_shape=input_2d))\n",
    "        model.add(MaxPooling2D(pool_size=pool_2d, name='pool1'))\n",
    "\n",
    "        model.add(Convolution2D(8, 3, 3, name='conv2'))\n",
    "        model.add(MaxPooling2D(pool_size=pool_2d, name='pool2'))\n",
    "\n",
    "        model.add(Convolution2D(8, 3, 3, name='conv3'))\n",
    "        model.add(MaxPooling2D(pool_size=pool_2d, name='pool3'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    if(dimension == '3d'):\n",
    "        model = build_conv_3d()\n",
    "    else:\n",
    "        model = build_conv_2d()\n",
    "        \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2000, activation='relu', name='dense1'))\n",
    "    model.add(Dropout(0.5, name='dropout1'))\n",
    "\n",
    "    model.add(Dense(500, activation='relu', name='dense2'))\n",
    "    model.add(Dropout(0.5, name='dropout2'))\n",
    "\n",
    "    model.add(Dense(nb_classes, activation=activation, name='softmax'))\n",
    "\n",
    "    if w_path:\n",
    "        model.load_weights(w_path)\n",
    "\n",
    "    opt = keras.optimizers.Adadelta(clipnorm=1.)\n",
    "    \n",
    "    if(compile_model):\n",
    "        model.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    print ('Done building model.')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv3D` call to the Keras 2 API: `Conv3D(8, (3, 3, 3), name=\"conv1\", input_shape=(192, 192,...)`\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv3D` call to the Keras 2 API: `Conv3D(8, (3, 3, 3), name=\"conv2\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv3D` call to the Keras 2 API: `Conv3D(8, (3, 3, 3), name=\"conv3\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done building model.\n"
     ]
    }
   ],
   "source": [
    "model = build_cnn(dimension = '3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracks model learning rate\n",
    "class SGDLearningRateTracker(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        optimizer = self.model.optimizer\n",
    "        lr = K.eval(optimizer.lr * (1. / (1. + optimizer.decay * optimizer.iterations)))\n",
    "        print (str('\\nLR: {:.6f}\\n').format(float(lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting model architecture to data, also runs loss/accuracy tracker\n",
    "def fit_model(model, v, train_data, train_labels, val_data, val_labels):\n",
    "    model_weights_file = 'img_classifier_weights_%s.h5' %v\n",
    "    epoch_weights_file = 'img_classifier_weights_%s_{epoch:02d}_{val_acc:.2f}.hdf5' %v\n",
    "    model_file = 'img_classifier_model_%s.h5' %v\n",
    "    history_file = 'img_classifier_history_%s.json' %v\n",
    "    \n",
    "    def save_model_and_weights():\n",
    "        model.save(model_file)\n",
    "        model.save_weights(model_weights_file)\n",
    "        \n",
    "        return 'Saved model and weights to disk!'\n",
    "\n",
    "    def save_model_history(m):\n",
    "        with open(history_file, 'wb') as history_json_file:\n",
    "            json.dump(m.history, history_json_file)\n",
    "        \n",
    "        return 'Saved model history to disk!'\n",
    "    \n",
    "    def visualise_accuracy(m):\n",
    "        plt.plot(m.history['acc'])\n",
    "        plt.plot(m.history['val_acc'])\n",
    "        plt.title('model accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'validation'], loc='upper left')\n",
    "        plt.show()\n",
    "      \n",
    "    def visualise_loss(m):\n",
    "        plt.plot(m.history['loss'])\n",
    "        plt.plot(m.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'validation'], loc='upper left')\n",
    "        plt.show()\n",
    "    \n",
    "    def model_callbacks():\n",
    "        checkpoint = ModelCheckpoint(epoch_weights_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=early_stopping_patience, verbose=1, mode='auto')\n",
    "        lr_tracker = SGDLearningRateTracker()\n",
    "        \n",
    "        return [checkpoint,early_stopping,lr_tracker]\n",
    "        \n",
    "    callbacks_list = model_callbacks()\n",
    "\n",
    "    m = model.fit(train_data,train_labels,batch_size=batch_size,nb_epoch=nb_epoch,verbose=1,shuffle=True,validation_data=(val_data,val_labels),callbacks=callbacks_list)\n",
    "    \n",
    "    print (save_model_and_weights())\n",
    "    print (save_model_history(m))\n",
    "    \n",
    "    visualise_accuracy(m)\n",
    "    visualise_loss(m)\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates test data performance and creates confusion matrix\n",
    "def evaluate_model(m, weights, test_data, test_labels):    \n",
    "    def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "\n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, cm[i, j],\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "     \n",
    "    plt.close('all')\n",
    "\n",
    "    m.load_weights(weights)\n",
    "    m.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "    print (\"Done compiling model.\")\n",
    "    \n",
    "    prediction = m.predict(test_data)\n",
    "    prediction_labels = np_utils.to_categorical(np.argmax(prediction, axis=1), nb_classes)\n",
    "    \n",
    "    print ('Accuracy on test data:', accuracy_score(test_labels, prediction_labels))\n",
    "\n",
    "    print ('Classification Report')\n",
    "    print (classification_report(test_labels, prediction_labels, target_names = class_names))\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(np.argmax(test_labels, axis=1), np.argmax(prediction, axis=1))\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes = class_names, normalize=False, title='Confusion matrix')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:47: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450 samples, validate on 92 samples\n",
      "Epoch 1/25\n"
     ]
    }
   ],
   "source": [
    "#This will take time\n",
    "fit_model(model, \"v1\", train_data, train_labels, val_data, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the fit model and evaluate performance on test\n",
    "evaluate_model(model, 'img_classifier_weights_v1.h5', test_data, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

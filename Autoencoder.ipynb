{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1205 17:57:28.494312 139852354701056 deprecation.py:323] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    " \n",
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "# there was some problems for using import tensorflow as tf\n",
    "# the way I found to fix it is the next two lines\n",
    "import tensorflow.compat.v1 as tf \n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras import backend\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Activation, GlobalAveragePooling2D, Dense, GlobalAveragePooling3D\n",
    "from keras.layers import Input, Conv2D, ZeroPadding2D, BatchNormalization\n",
    "from keras.layers import MaxPooling3D, Flatten, Dense, Conv3D\n",
    "from keras.models import Model\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.utils import to_categorical\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "#reduce the size of model weights\n",
    "from keras import backend as K\n",
    "K.set_floatx('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListOfFiles(dirName):\n",
    "    # create a list of file and sub directories \n",
    "    # names in the given directory \n",
    "    listOfFile = os.listdir(dirName)\n",
    "    allFiles = list()\n",
    "    # Iterate over all the entries\n",
    "    for entry in listOfFile:\n",
    "        # Create full path\n",
    "        fullPath = os.path.join(dirName, entry)\n",
    "        # If entry is a directory then get the list of files in this directory \n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles = allFiles + getListOfFiles(fullPath)\n",
    "        elif fullPath[-4:]=='.nii': # to avoid DS_Store or other system files\n",
    "            allFiles.append(fullPath)\n",
    "                \n",
    "    return allFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImage( subject, directory ):\n",
    "\n",
    "    epi_img = nib.load(subject + '.nii')\n",
    "    epi_img_data = epi_img.get_fdata()\n",
    "        \n",
    "    return epi_img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract m 5*5*5 patch from the images and put them in 'data' variable\n",
    "\n",
    "#As we want to have 1000 patches from 100 MRI scanes\n",
    "m = 1000*10\n",
    "\n",
    "#flatten patches\n",
    "data = np.empty((m, 125))\n",
    "\n",
    "#You may want to change the following line to the folder in which files are stores\n",
    "directory = \"./ADNI_Train/\"\n",
    "files = getListOfFiles(directory)\n",
    "filenums = len(files)\n",
    "\n",
    "# We will skip the patches that are all zero ('bad' patches)\n",
    "num_added = 0\n",
    "\n",
    "xdim = 68\n",
    "ydim = 95\n",
    "zdim = 79\n",
    "\n",
    "#for loop will continue until m 'good' patches added to data\n",
    "for index in range(m):\n",
    "    if num_added >= m:\n",
    "        #all data is complete now\n",
    "        break\n",
    "    random = np.random.randint(filenums)\n",
    "    filename = files[random]\n",
    "    subject = filename[:-4]\n",
    "    image = getImage(subject=subject, directory=directory)\n",
    "    \n",
    "    n_i, n_j, n_k = image.shape\n",
    "        \n",
    "    if (n_i != xdim or n_j != ydim or n_k != zdim):\n",
    "        #epi_img_data = cv2.resize(epi_img_data, (192, 192))\n",
    "        #epi_img_data = epi_img_data[0:192,0:192,int(n_k/2)-80:int(n_k/2)+80]\n",
    "            \n",
    "        #resampling to make all MRI volumes the same dimensions\n",
    "        image = zoom(image, (float(xdim/n_i), float(ydim/n_j), float(zdim/n_k)), order = 0)\n",
    "    \n",
    "    #about 10 patches from each image\n",
    "    for i in range(10):\n",
    "        n_i, n_j, n_k = image.shape\n",
    "        rand = np.random.rand(3)\n",
    "    \n",
    "        corner_i = np.random.randint(n_i-5)\n",
    "        corner_j = np.random.randint(n_j-5)\n",
    "        corner_k = np.random.randint(n_k-5)\n",
    "        patch = image[corner_i:corner_i+5,corner_j:corner_j+5,corner_k:corner_k+5]\n",
    "        patch = patch.flatten()\n",
    "        if np.linalg.norm(patch)!=0:\n",
    "            data[num_added,:] = patch\n",
    "#             print(data[index,:])\n",
    "            num_added += 1\n",
    "#             print('added', num_added)\n",
    "            if num_added >= m:\n",
    "                break\n",
    " \n",
    "    \n",
    "# Normalize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(data[:int(0.7*m)])\n",
    "X_train = np.transpose(scaler.fit_transform(data[:int(0.7*m)]))\n",
    "X_test =  np.transpose(scaler.fit_transform(data[int(0.7*m):]))\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing parameters\n",
    "n_inputs = 125\n",
    "n_hidden = 150\n",
    "n_outputs = 125\n",
    "\n",
    "def initialize_parameters():\n",
    "    \n",
    "    \n",
    "        \n",
    "    ### START CODE HERE ### (approx. 6 lines of code)\n",
    "    W1 = tf.get_variable(\"W1\", [n_hidden,n_inputs])\n",
    "    b1 = tf.get_variable(\"b1\",[n_hidden,1],initializer=tf.zeros_initializer())\n",
    "    b2 = tf.get_variable(\"b2\",[n_outputs,1],initializer=tf.zeros_initializer())\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    parameters = initialize_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "#retrive parameters\n",
    "W1 = parameters['W1']\n",
    "b1 = parameters['b1']\n",
    "b2 = parameters['b2']\n",
    "\n",
    "\n",
    "learning_rate = 0.005\n",
    "\n",
    "#sparsity hyper-parameter (It is not the best amount)\n",
    "s=0.05\n",
    "\n",
    "\n",
    "#penalty for sparsity (It is not the best amount)\n",
    "beta = 2\n",
    "\n",
    "#lambda, hyperparameter for weights (It is not the best amount)\n",
    "lamb = 100\n",
    "\n",
    "# Define architecture of autoencoder\n",
    "X = tf.placeholder(tf.float32, shape=[n_inputs, None])\n",
    "Z1 = tf.add(tf.matmul(W1,X),b1)\n",
    "\n",
    "#regarding the paper first layer activation is sigmoid\n",
    "hidden = tf.nn.sigmoid(Z1)\n",
    "\n",
    "#regarding the paper second activation is identity\n",
    "#regarding the paper the kernel for second activation is transpose of the first layer kernel\n",
    "outputs = tf.add(tf.matmul(tf.transpose(W1),hidden),b2) \n",
    "\n",
    "# calculating kullback leibler loss\n",
    "Shat = tf.reduce_mean(hidden,axis=1)\n",
    "KL1 = s*(tf.log(s)-tf.log(Shat))+(1-s)*(tf.log(1-s)-tf.log(1-Shat))\n",
    "KL = tf.math.reduce_sum(KL1)\n",
    "\n",
    "#calculating norm loss\n",
    "weight_L2_norm = tf.reduce_mean(tf.square(W1))\n",
    "\n",
    "#calculating similarity loss\n",
    "similoss = tf.reduce_mean(tf.reduce_sum(tf.square(outputs - X),axis=0))/2 #Is the axis=0 correct for reduce_sum?\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss = similoss + beta*KL + lamb*weight_L2_norm\n",
    "\n",
    "#defining optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To rerun this you need to run the last three blocks also\n",
    "\n",
    "#initialization\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_iterations = 5000\n",
    "costs = []\n",
    "test_costs = []\n",
    "random_costs = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        _ , cost = sess.run([training_op,loss],feed_dict={X:X_train})\n",
    "        \n",
    "        costs.append(cost)\n",
    "#         training_op.run(feed_dict={X: X_train})\n",
    "        \n",
    "            \n",
    "#         train_loss=loss.eval(feed_dict={X:X_train})\n",
    "#         print(train_loss)\n",
    "    \n",
    "    \n",
    "        test_loss=loss.eval(feed_dict={X:X_test})\n",
    "        test_costs.append(test_loss)\n",
    "           \n",
    "    \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs[500:]))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per fives)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    # plot the test cost\n",
    "    plt.plot(np.squeeze(test_costs[500:]))\n",
    "    plt.ylabel('test cost')\n",
    "    plt.xlabel('iterations (per fives)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "#     # plot the test cost\n",
    "#     plt.plot(np.squeeze(random_costs))\n",
    "#     plt.ylabel('test cost')\n",
    "#     plt.xlabel('iterations (per fives)')\n",
    "#     plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "#     plt.show()\n",
    "    \n",
    "#     print(X_test.shape,'X_test shape')\n",
    "#     print(test_output.shape,'test output shape')\n",
    "#     print(tf.square(outputs - X).shape,'shape of tf.square')\n",
    "#     print('tf.square',tf.square(outputs - X).eval(feed_dict={X:X_test}))\n",
    "#     print('equal',(test_output-X_test)*(test_output-X_test))\n",
    "#     print(tf.reduce_mean(tf.square(outputs - X)).shape,'reduce mean shape')\n",
    "    \n",
    "    final_train_loss=loss.eval(feed_dict={X:X_train})\n",
    "    \n",
    "    print('train_loss',final_train_loss)\n",
    "    \n",
    "    similoss1 = similoss.eval(feed_dict={X:X_train})\n",
    "    weight_L2_norm1 = weight_L2_norm.eval(feed_dict={X:X_train})\n",
    "    KL1 = KL.eval(feed_dict={X:X_train})\n",
    "    \n",
    "\n",
    "    # lets save the parameters in a tensor variable, we may need trained parameters to use for the next part of DL\n",
    "    #parameters include W1, b1, b2; however, we will just use W1 and b1\n",
    "    parameters_tf = sess.run(parameters)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is to check what is the contribution of each part in loss to adjust lamb and beta\n",
    "print(similoss1)\n",
    "print(weight_L2_norm1*lamb)\n",
    "print(beta*KL1)\n",
    "print(similoss1+weight_L2_norm1*lamb+beta*KL1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_shape):\n",
    "    # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    X_conv = Conv3D(150, (5, 5, 5), strides = (1, 1,1), name = 'conv0',data_format=\"channels_last\", activation='sigmoid',trainable=False)(X_input)\n",
    "    \n",
    "    X_pool = MaxPooling3D(pool_size=(5,5,5), strides=None, data_format=None, name = 'pool')(X_conv) # it should be 5,5,5,\n",
    "    \n",
    "    X_pool_flat = Flatten()(X_pool)\n",
    "    \n",
    "    X_dense = Dense(100, activation='sigmoid', name='fc')(X_pool_flat) # 100 should be 800 in the paper\n",
    "    \n",
    "    X_out = Dense(1, activation='sigmoid', name='out')(X_dense)\n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = X_out, name='HappyModel')    \n",
    "    \n",
    "    \n",
    "#     model.load_weights()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1128 05:52:57.079346 139684368774912 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1128 05:52:57.080569 139684368774912 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1128 05:52:57.083907 139684368774912 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1128 05:52:57.133712 139684368774912 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1128 05:52:57.134554 139684368774912 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model1 = model ((68,95,79,1))\n",
    "a = model1.get_weights()\n",
    "#set weights of convolution layer\n",
    "W1=parameters_tf['W1']\n",
    "W1=W1.T.reshape(np.shape(a[0]))\n",
    "# new_weights1 = []\n",
    "# new_weights1.append(W1)\n",
    "a[0] = W1\n",
    "b1=parameters_tf['b1']\n",
    "b1 = b1.reshape(np.shape(a[1]))\n",
    "# new_weights1.append(b1)\n",
    "a[1] = b1\n",
    "model1.set_weights(a)\n",
    "# trainable=False\n",
    "#save model weights, note that we will only use the weights for the convolution layer\n",
    "model1.save_weights('my_model_weights_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17162476"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][0][0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 5, 1, 150) (150,) (56700, 400) (400,) (400, 3) (3,)\n"
     ]
    }
   ],
   "source": [
    "a = model1.get_weights()\n",
    "print(a[0].shape,a[1].shape,a[2].shape,a[3].shape,a[4].shape,a[5].shape,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is just a double check to make sure TF model properly transfered to Keras model;however, we do not nedd it for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(input_shape):\n",
    "    # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    X = Conv3D(150, (5, 5, 5), strides = (1, 1,1), name = 'conv0',data_format=\"channels_last\")(X_input)\n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = X, name='HappyModel')\n",
    "        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 18750 into shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-e577fd89d701>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mW1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mW1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnew_weights1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnew_weights1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 18750 into shape (1,)"
     ]
    }
   ],
   "source": [
    "#Using autoencoder to have 150 cubes of size 64*91*75 from the main input of size 68*95*79\n",
    "files[1]\n",
    "subject = filename[:-4]\n",
    "image = getImage(subject=subject, directory=directory)\n",
    "model_test1 = model_test ((68,95,79,1))\n",
    "a=model_test1.get_weights()\n",
    "W1=parameters['W1']\n",
    "W1=W1.T.reshape(np.shape(a[0]))\n",
    "new_weights1 = []\n",
    "new_weights1.append(W1)\n",
    "b1=parameters['b1']\n",
    "b1 = b1.reshape(np.shape(a[1]))\n",
    "new_weights1.append(b1)\n",
    "model_test1.set_weights(new_weights1)\n",
    "x_test = epi_img_data.reshape(1,68,95,79,1)\n",
    "output = model_test1.predict(x_test)\n",
    "outputKeras= output[0,0,0,0,:].reshape(150,1)\n",
    "#now comparing that with tf output\n",
    "a = x_test[0,30:35,30:35,30:35,0].flatten().reshape(125,1)\n",
    "W1 = parameters['W1']\n",
    "outputTF = np.matmul(W1,a)+b1.reshape(150,1)\n",
    "print((output[0,30,30,30,:].reshape(150,1)-outputTF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

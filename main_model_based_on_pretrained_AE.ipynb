{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1207 22:46:35.583787 140130305517312 deprecation.py:323] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    " \n",
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "# there was some problems for using import tensorflow as tf\n",
    "# the way I found to fix it is the next two lines\n",
    "import tensorflow.compat.v1 as tf \n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import random\n",
    "\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Activation, GlobalAveragePooling2D, Dense, GlobalAveragePooling3D\n",
    "from tensorflow.keras.layers import Input, Conv2D, ZeroPadding2D, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv3D,MaxPooling3D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "#reduce the size of model weights\n",
    "from tensorflow.keras import backend as K\n",
    "K.set_floatx('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#maybe I do not need this in future\n",
    "def getListOfFiles(dirName):\n",
    "    # create a list of file and sub directories \n",
    "    # names in the given directory \n",
    "    listOfFile = os.listdir(dirName)\n",
    "    allFiles = list()\n",
    "    # Iterate over all the entries\n",
    "    for entry in listOfFile:\n",
    "        # Create full path\n",
    "        fullPath = os.path.join(dirName, entry)\n",
    "        # If entry is a directory then get the list of files in this directory \n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles = allFiles + getListOfFiles(fullPath)\n",
    "        elif fullPath[-4:]=='.nii': # to avoid DS_Store or other system files\n",
    "            allFiles.append(fullPath)\n",
    "                \n",
    "    return allFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_shape):\n",
    "    # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "#     X_conv = Conv3D(150, (5, 5, 5), strides = (1, 1,1), name = 'conv0', data_format=\"channels_last\", activation='sigmoid', trainable=False)(X_input)\n",
    "    X_conv = Conv3D(30, (5, 5, 5), strides = (1, 1,1), name = 'conv0', data_format=\"channels_last\", activation='sigmoid')(X_input)\n",
    "    \n",
    "    X_conv = Conv3D(30, (5, 5, 5), strides = (1, 1,1), name = 'conv0', data_format=\"channels_last\", activation='sigmoid')(X_input)\n",
    "\n",
    "    X_pool = MaxPooling3D(pool_size=(5, 5, 5), strides=None, data_format=None, name = 'pool')(X_conv) # in the paper it is 5,5,5\n",
    "    \n",
    "    X_pool_flat = Flatten()(X_pool)\n",
    "    \n",
    "    X_dense = Dense(200, activation='sigmoid', name='fc')(X_pool_flat) #400 is 800 in the paper\n",
    "    \n",
    "    X_out = Dense(1, activation='sigmoid', name='out')(X_dense)\n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = X_out, name='HappyModel')    \n",
    "    \n",
    "    \n",
    "#     model.load_weights()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1207 22:46:47.365651 140130305517312 deprecation.py:506] From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model1 = model ((110,110,110,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model weights, note that we will only use the weights for the convolution layer\n",
    "model1.load_weights('my_model_weights_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"HappyModel\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 110, 110, 110, 1) 0         \n",
      "_________________________________________________________________\n",
      "conv0 (Conv3D)               (None, 106, 106, 106, 150 18900     \n",
      "_________________________________________________________________\n",
      "pool (MaxPooling3D)          (None, 21, 21, 21, 150)   0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1389150)           0         \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 200)               277830200 \n",
      "_________________________________________________________________\n",
      "out (Dense)                  (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 277,849,301\n",
      "Trainable params: 277,849,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters (Modify as needed)\n",
    "img_size_x = 110\n",
    "img_size_y = 110\n",
    "img_size_z = 110\n",
    "\n",
    "batch_size = 128\n",
    "nb_classes = 3\n",
    "nb_epoch = 25\n",
    "\n",
    "c = 0\n",
    "\n",
    "learning_rate = 0.001\n",
    "early_stopping_patience = 20\n",
    "\n",
    "class_names = [\"CN\", \"MCI\", \"AD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DATA CLEANING\n",
    "\n",
    "# # Get folder of nii files only - find Documents/CS230/MRI_git/ADNI -\n",
    "# # type f -print0 | xargs -0 mv -t Documents/CS230/MRI_git/ADNI_Clean/\n",
    "\n",
    "# # Change names of nii files\n",
    "# directory = \"./../../../../../Documents/Stanford/cs230/ADNI/ADNI_Clean/\"\n",
    "# for filename in os.listdir(directory):\n",
    "#     if filename.startswith(\"ADNI\"):\n",
    "#         os.rename(directory+filename, directory+filename[5:16]+\".nii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImage( subject, directory ):\n",
    "\n",
    "    epi_img = nib.load(directory + subject + '.nii')\n",
    "    epi_img_data = epi_img.get_fdata()\n",
    "        \n",
    "    return epi_img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_dataset(directory):\n",
    "    \n",
    "#     patientData = np.loadtxt(\"ADNI1_Complete_1Yr_1.5T_10_26_2019.csv\", dtype= 'str', skiprows=1, delimiter=',')\n",
    "    \n",
    "#     integer_mapping = {x: i for i,x in enumerate(['AD', 'MCI', 'CN'])}\n",
    "#     y = np.asarray([integer_mapping[word] for word in patientData[:,1]])\n",
    "#     labels = to_categorical(y, num_classes=3, dtype='float32')\n",
    "    \n",
    "    \n",
    "    \n",
    "#     xdim = 68\n",
    "#     ydim = 95\n",
    "#     zdim = 79\n",
    "#     X = np.zeros((637,xdim,ydim,zdim,1))\n",
    "#     Y = np.zeros((637,3))\n",
    "#     names = []\n",
    "#     for i, filename in enumerate(os.listdir(directory)):\n",
    "#         if i == 150:\n",
    "#             break\n",
    "#         if filename[-4:]!='.nii':\n",
    "#             continue\n",
    "        \n",
    "#         subject = filename[:-4]\n",
    "        \n",
    "#         # Get voxel array\n",
    "#         epi_img_data = getImage( subject, directory )\n",
    "        \n",
    "#         n_i, n_j, n_k = epi_img_data.shape\n",
    "        \n",
    "#         if (n_i != xdim or n_j != ydim or n_k != zdim):\n",
    "#             #epi_img_data = cv2.resize(epi_img_data, (192, 192))\n",
    "#             #epi_img_data = epi_img_data[0:192,0:192,int(n_k/2)-80:int(n_k/2)+80]\n",
    "            \n",
    "#             #resampling to make all MRI volumes the same dimensions\n",
    "#             epi_img_data = zoom(epi_img_data, (float(xdim/n_i), float(ydim/n_j), float(zdim/n_k)), order = 0)\n",
    "        \n",
    "#         x = epi_img_data\n",
    "        \n",
    "#         x = np.expand_dims(x, axis=3)\n",
    "        \n",
    "#         X[i] = x\n",
    "#         ind = np.where(patientData[:,0] == subject[:-1])[0][0]\n",
    "#         y = labels[ind, :]\n",
    "#         y = y.reshape(1,-1)\n",
    "#         names.append(subject)\n",
    "        \n",
    "#         Y[i] = y\n",
    "        \n",
    "#         if i % 50 == 0:\n",
    "#             print (\"loaded \" + str(i) + \"subjects\")\n",
    "        \n",
    "        \n",
    "#     print (X.shape)\n",
    "#     print (Y.shape)\n",
    "        \n",
    "#     return X, Y, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_clean():\n",
    "    AD_directory = \"/home/ubuntu/project/Data/AD_clean/\"\n",
    "    NC_directory = \"/home/ubuntu/project/Data/NC_clean/\"\n",
    "    \n",
    "    AD_list = os.listdir(AD_directory)\n",
    "    NC_list = os.listdir(NC_directory)\n",
    "    \n",
    "\n",
    "    All_directory = \"/home/ubuntu/project/Data/AD_NC_Clean/\"\n",
    "\n",
    "    X = np.zeros((111,img_size_x,img_size_y,img_size_z,1))\n",
    "    Y = np.zeros(111)\n",
    "    \n",
    "    l = os.listdir(All_directory)\n",
    "    random.shuffle(l)\n",
    "   \n",
    "    \n",
    "    objindex = 0\n",
    "    for i, filename in enumerate(l):\n",
    "        if (filename.startswith('.')):\n",
    "            print (\"hidden file\")\n",
    "        else:\n",
    "            epi_img = nib.load(All_directory + filename)\n",
    "            x = epi_img.get_fdata()\n",
    "            x = (x-x.min())/x.max()\n",
    "            \n",
    "            x = np.expand_dims(x, axis=3)\n",
    "            X[objindex] = x\n",
    "            y = 0\n",
    "            if filename in AD_list:\n",
    "                y = 1\n",
    "                \n",
    "            Y[objindex] = y\n",
    "            \n",
    "            objindex += 1\n",
    "            \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_dataset_clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111,)\n",
      "(111, 110, 110, 110, 1)\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "70/70 [==============================] - 25s 359ms/sample - loss: 0.7073 - acc: 0.4570 - val_loss: 0.7169 - val_acc: 0.4666\n",
      "Epoch 2/20\n",
      "70/70 [==============================] - 25s 352ms/sample - loss: 0.7019 - acc: 0.4570 - val_loss: 0.7132 - val_acc: 0.4666\n",
      "Epoch 3/20\n",
      "70/70 [==============================] - 25s 353ms/sample - loss: 0.6988 - acc: 0.4570 - val_loss: 0.7120 - val_acc: 0.4666\n",
      "Epoch 4/20\n",
      "70/70 [==============================] - 25s 354ms/sample - loss: 0.6967 - acc: 0.4570 - val_loss: 0.7085 - val_acc: 0.4666\n",
      "Epoch 5/20\n",
      "70/70 [==============================] - 25s 356ms/sample - loss: 0.6947 - acc: 0.4714 - val_loss: 0.7081 - val_acc: 0.4666\n",
      "Epoch 6/20\n",
      "70/70 [==============================] - 25s 354ms/sample - loss: 0.6940 - acc: 0.4570 - val_loss: 0.7062 - val_acc: 0.4666\n",
      "Epoch 7/20\n",
      "70/70 [==============================] - 25s 354ms/sample - loss: 0.6918 - acc: 0.4570 - val_loss: 0.7060 - val_acc: 0.4666\n",
      "Epoch 8/20\n",
      "70/70 [==============================] - 25s 353ms/sample - loss: 0.6899 - acc: 0.4856 - val_loss: 0.7043 - val_acc: 0.4666\n",
      "Epoch 9/20\n",
      "70/70 [==============================] - 25s 352ms/sample - loss: 0.6898 - acc: 0.4856 - val_loss: 0.7033 - val_acc: 0.4666\n",
      "Epoch 10/20\n",
      "70/70 [==============================] - 25s 355ms/sample - loss: 0.6883 - acc: 0.5000 - val_loss: 0.7025 - val_acc: 0.4666\n",
      "Epoch 11/20\n",
      "70/70 [==============================] - 25s 355ms/sample - loss: 0.6877 - acc: 0.4856 - val_loss: 0.7034 - val_acc: 0.4666\n",
      "Epoch 12/20\n",
      "70/70 [==============================] - 25s 352ms/sample - loss: 0.6855 - acc: 0.4856 - val_loss: 0.7021 - val_acc: 0.4333\n",
      "Epoch 13/20\n",
      "70/70 [==============================] - 25s 352ms/sample - loss: 0.6846 - acc: 0.4856 - val_loss: 0.7007 - val_acc: 0.4333\n",
      "Epoch 14/20\n",
      "70/70 [==============================] - 25s 352ms/sample - loss: 0.6837 - acc: 0.4856 - val_loss: 0.7005 - val_acc: 0.4333\n",
      "Epoch 15/20\n",
      "70/70 [==============================] - 25s 354ms/sample - loss: 0.6831 - acc: 0.4856 - val_loss: 0.6997 - val_acc: 0.3999\n",
      "Epoch 16/20\n",
      "70/70 [==============================] - 25s 354ms/sample - loss: 0.6824 - acc: 0.5142 - val_loss: 0.7001 - val_acc: 0.3333\n",
      "Epoch 17/20\n",
      "70/70 [==============================] - 25s 354ms/sample - loss: 0.6832 - acc: 0.5288 - val_loss: 0.6970 - val_acc: 0.4666\n",
      "Epoch 18/20\n",
      "70/70 [==============================] - 25s 352ms/sample - loss: 0.6824 - acc: 0.6855 - val_loss: 0.6993 - val_acc: 0.4666\n",
      "Epoch 19/20\n",
      "70/70 [==============================] - 25s 354ms/sample - loss: 0.6818 - acc: 0.6143 - val_loss: 0.6976 - val_acc: 0.4666\n",
      "Epoch 20/20\n",
      "70/70 [==============================] - 25s 353ms/sample - loss: 0.6800 - acc: 0.6572 - val_loss: 0.6984 - val_acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "#load model weights, note that we will only use the weights for the convolution layer\n",
    "from tensorflow.keras import losses\n",
    "# model1.load_weights('my_model_weights_0_14.h5')\n",
    "# opt = keras.optimizers.Adadelta(clipnorm=1.)\n",
    "opt = tf.keras.optimizers.Adam(lr=0.00005,beta_1=0.9, beta_2=0.99, amsgrad=False)\n",
    "# def batch_callback():\n",
    "#     print('hello')\n",
    "    \n",
    "model1.compile(optimizer=opt,loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model1.compile(loss=losses.mean_squared_error, optimizer='sgd', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "m = model1.fit(X[0:70],Y[0:70],validation_data=(X[70:100],Y[70:100]),batch_size=2,epochs=20, verbose=1)\n",
    "# model1.train_on_batch(X[0:10], Y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save_weights('my_model_weights_0_1398.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.466e-05 9.971e-01 2.689e-03]\n",
      " [5.327e-01 4.663e-01 9.298e-04]\n",
      " [2.548e-02 2.588e-02 9.487e-01]\n",
      " [1.000e+00 3.576e-07 4.053e-06]\n",
      " [5.327e-01 4.663e-01 9.298e-04]\n",
      " [9.058e-01 9.436e-02 6.974e-06]\n",
      " [5.264e-01 4.729e-01 8.540e-04]\n",
      " [4.768e-07 9.995e-01 4.499e-04]\n",
      " [2.548e-02 2.588e-02 9.487e-01]\n",
      " [1.478e-05 9.971e-01 2.701e-03]]\n"
     ]
    }
   ],
   "source": [
    "classes = model1.predict(X[0:10])\n",
    "# -np.sum(np.log(classes)*Y[0:10])/10\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(Y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "10/10 [==============================] - 0s 46ms/sample - loss: 5.0312 - categorical_accuracy: 0.2000\n"
     ]
    }
   ],
   "source": [
    "x_test = X[0:10]\n",
    "y_test = Y[0:10]\n",
    "loss_and_metrics = model1.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "print(type(loss_and_metrics[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96337890625\n"
     ]
    }
   ],
   "source": [
    "print(loss_and_metrics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(loss_and_metrics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2344 0.4507 0.315 ]\n",
      " [0.2335 0.4526 0.314 ]\n",
      " [0.2129 0.4749 0.3123]\n",
      " [0.2134 0.4727 0.314 ]\n",
      " [0.2313 0.451  0.3179]\n",
      " [0.2242 0.4683 0.3074]\n",
      " [0.4214 0.343  0.2357]\n",
      " [0.2327 0.452  0.3154]\n",
      " [0.2133 0.4746 0.3123]\n",
      " [0.2131 0.474  0.3127]]\n"
     ]
    }
   ],
   "source": [
    "classes = model1.predict(X[0:10])\n",
    "# -np.sum(np.log(classes)*Y[0:10])/10\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1117 9.86   0.0399]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(classes,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 5. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(Y[90:100],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " - 4s - loss: 1.0896 - categorical_accuracy: 0.3899\n",
      "Epoch 2/3\n",
      " - 3s - loss: 1.0803 - categorical_accuracy: 0.3200\n",
      "Epoch 3/3\n",
      " - 4s - loss: 1.0703 - categorical_accuracy: 0.3700\n"
     ]
    }
   ],
   "source": [
    "m = model1.fit(X[100:200],Y[100:200],batch_size=20,epochs=3, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = model1.predict(X[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8fc3e26b30dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'classes' is not defined"
     ]
    }
   ],
   "source": [
    "classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelpoly():\n",
    "    # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    X_dense1 = Dense(100, activation='sigmoid', name='fc')(X_input) #400 is 800 in the paper\n",
    "    \n",
    "    X_dense1 = Dense(100, activation='sigmoid', name='fc')(X_input)\n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = X_out, name='HappyModel')    \n",
    "    \n",
    "    \n",
    "#     model.load_weights()\n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
